\chapter{Literature Review and Related Work}
\label{chap:relatedworks}

\section{Competitor Analysis}
\label{section:competitor-analysis}

\begin{table}[htbp!]
   \begin{adjustwidth}{-.85in}{-.85in}
       \noindent
       \centering
       \small\begin{tabularx}{1.3\textwidth}{|X|>{\columncolor{green!20}}X|X|X|X|}
           \hline & \textbf{\usevar{\srsTitle}} & \textbf{Hikvision} & \textbf{YOLORe-IDNet} & \textbf{JARVIS-MoCap} \\\hline
           \textbf{Primary Focus} & Campus tracking with persistent identity & Single-camera surveillance with minimal cross-camera capability & Real-time detection without integrated systems & Research motion capture in controlled settings \\\hline
           \textbf{Core Technology} & Multi-model fusion with spatial mapping & Proprietary algorithms optimized for single views & YOLO detection with separate Re-ID network & Markerless pose estimation with synced cameras \\\hline
           \textbf{Multi-Camera Detection} & Independent detection robust to occlusions & Strong within-view detection, poor across views & No multi-camera management system & Requires calibrated camera arrays \\\hline
           \textbf{Tracking and Re-ID} & Identity across non-overlapping views & Excellent within single views, fails across views & Matches appearance without trajectories & Tracks pose, not identity \\\hline
           \textbf{Spatial Representation} & Unified coordinate system visualization & Limited to individual camera views & No spatial mapping functionality & Limited to pre-calibrated environments \\\hline
           \textbf{System Accessibility} & Web interface with easy deployment & Requires dedicated hardware & Command-line interface for experts & Research software with complex setup \\\hline
           \textbf{Cross-Platform Support} & Any device with browser access & Specific hardware and OS requirements & Platform-dependent implementation & Desktop with specific dependencies \\\hline
       \end{tabularx}
   \end{adjustwidth}
   \caption{Competitor Analysis of \usevar{\srsTitle}}
\end{table}

In order to acknowledge the existing technology with similar functionalities, a competitor analysis is conducted.
There are various systems in the market that can track individuals, we have selected 3 of which most closely resemble our project:

\begin{itemize}
   \item \textbf{\textit{Hikvision AI Camera Systems}}---a commercial leader in the Thailand AI camera market offering excellent single-camera tracking but limited cross-camera capabilities.
   \item \textbf{\textit{YOLORe-IDNet}}---a research implementation combining YOLO object detection with re-identification networks for real-time tracking without prior knowledge.
   \item \textbf{\textit{JARVIS-MoCap}}---an open-source markerless 3D motion capture system designed for research environments requiring precise tracking.
\end{itemize}
\par

As shown in Table 2.1, each system possesses distinct strengths and limitations across key performance areas. Hikvision excels in proprietary single-camera tracking but struggles with cross-camera identity persistence. YOLORe-IDNet offers real-time detection capabilities but lacks integrated systems and spatial mapping functionality. JARVIS-MoCap provides precise markerless pose estimation but requires controlled environments with calibrated camera arrays and complex setup procedures. All competitors demonstrate significant limitations in maintaining unified tracking across non-overlapping views, implementing accessible user interfaces, or providing flexible deployment options across various platformsâ€”capabilities that are essential for comprehensive campus security applications.

The \usevar{\srsTitle} stands apart from competitors through its unique combination of persistent identity tracking across non-overlapping camera views and blind spots, a capability not found in Hikvision's proprietary hardware-dependent solution which excels at single-camera tracking but fails across non-overlapping views with time gaps. Unlike YOLORe-IDNet's research implementation which lacks continuous identity maintenance, our system preserves identity even when subjects disappear temporarily. JARVIS-MoCap requires controlled environments and specialized equipment, while our system works with existing camera infrastructure and adapts to varying lighting conditions through self-calibrating algorithms. Our multi-model fusion approach ensures reliable tracking even in campus blind spots where other systems lose track. The system's interactive 3D mapping provides security personnel with comprehensive spatial awareness that surpasses the limited single-view representations of competitors, making it uniquely suited for complex campus environments where maintaining continuous identity is critical for security operations.

\section{Literature Review}
\label{section:literature-review}

The development of our multi-camera person tracking system builds upon established research while making specific architectural decisions to address the limitations of alternative approaches found in recent literature. This review is organized into two categories: foundational works that we directly leverage, and alternative methodologies that we analyzed but ultimately rejected in favor of our proposed pipeline.

\subsection{Foundational Works Leveraged}
\label{subsection:leveraged-works}
Our system integrates several state-of-the-art components and datasets that provide the necessary building blocks for robust tracking.

\paragraph{ByteTrack (Zhang et al. \cite{bytetrack})}
We leverage ByteTrack's "track-everything" philosophy, which associates low-confidence detection boxes rather than discarding them. This foundational work forms the core of our intra-camera tracking module, allowing our system to maintain trajectory continuity through brief occlusions or lighting changes without the need for complex post-hoc repairs.

\paragraph{FastReID (He et al. \cite{fastreid})}
For appearance modeling, we utilize the FastReID toolbox. Its modular architecture and powerful backbone networks provide the discriminative feature extraction capabilities essential for our Re-ID stage. We integrate this strictly as a feature extractor within our larger pipeline, separating it from the tracking logic to ensure modularity.

\paragraph{MTMMC Dataset (Woo et al. \cite{mtmmc})}
The MTMMC dataset serves as our primary training and benchmarking ground. Its diverse real-world scenarios (campus and factory) with multi-modal data provide the realistic conditions necessary to fine-tune our detection and Re-ID models for the specific challenges of our deployment environment.

\paragraph{Modular 3D Tracking (Bredereke et al. \cite{modular3d})}
We adopt the conceptual framework of modular 3D tracking using RGB cameras. While Bredereke et al. focus on synchronization, we leverage the core idea of mapping 2D tracks to 3D space as a distinct pipeline stage, which informs our Space-based Matching module.

\subsection{Key Insights and Adaptations from Recent Methodologies}
\label{subsection:comparative-analysis}
To ensure our system addresses the complexities of real-world deployment, we analyzed several recent alternative methodologies. Rather than viewing these as competing solutions, we treated them as sources of critical insight, adapting and evolving their core concepts to better suit our modular, privacy-focused architecture.

\subsubsection{From Unified Architectures to Modular Decoupling}
\textbf{Insight from Literature:} Gautam et al. \cite{yoloreidnet} demonstrated with YOLORe-IDNet that detection and re-identification tasks are deeply interconnected and can be effectively solved within a single computational pass.
\par
\textbf{Our Adaptation:} We learned from the efficiency of this coupling but refined the implementation into a \textbf{Decoupled Architecture} (using RT-DETR and FastReID as separate modules). This evolution allows us to retain the benefit of specialized feature extraction while gaining system modularity, enabling us to upgrade the detection engine (e.g., to newer Transformer models) without disrupting the re-identification gallery---a critical requirement for long-term system maintainability.

\subsubsection{From Reactive Repair to Proactive Prevention}
\textbf{Insight from Literature:} Wang et al. \cite{mixed_syn_tracking} rely on a post-hoc cleanup strategy. They allow tracklets to contain mixed identities and subsequently analyze the feature history using Gaussian Mixture Models (GMM). If the GMM detects multi-modal feature distributions (indicating two distinct persons under one ID), the system splits the track.
\par
\textbf{Our Adaptation:} We analyzed that these ID switches typically originate from track fragmentation during low-confidence periods (e.g., occlusion). We improved robustness by prioritizing \textbf{Proactive Prevention}. Integrating ByteTrack allows us to associate low-confidence detections, effectively bridging these gaps. Since the track continuity is maintained through the "dip" in confidence, the need for re-identification---and the risk of swapping IDs---is eliminated at the source.

\subsubsection{From Conditional Geometry to Architectural Overlap}
\textbf{Insight from Literature:} Wu et al. \cite{geom_consist_tracking} proved that 3D geometric constraints are a powerful fallback when visual appearance is unreliable due to occlusion. they implemented this as a conditional state that toggles on during occlusion.
\par
\textbf{Our Adaptation:} We validated the importance of geometry but advanced the implementation by making it the \textbf{Primary Architectural Bridge} (Space-Based Matching) rather than a conditional fallback. By continuously projecting all tracks to a unified 3D map, our system naturally bridges occlusions through multi-view spatial overlap, ensuring that geometric consistency is an inherent property of the system rather than a triggered state.

\subsubsection{From Trajectory Clustering to Sequential Filtering}
\textbf{Insight from Literature:} Hachiuma et al. \cite{hospital_tracking} showed that in environments with uniform clothing, physical trajectory provided a more reliable signal than visual features, suggesting that appearance can sometimes be a distraction.
\par
\textbf{Our Adaptation:} We synthesized this insight with visual matching using a \textbf{Sequential Filtering} approach. Our pipeline applies \textbf{Space-Based Matching} continuously to all tracks, projecting them to a unified map to handle overlapping fields and maintain global consistency. \textbf{Re-Identification} is triggered \textit{conditionally}---only when a valid spatial link suggests a cross-camera transition. This allows us to handle indistinguishable uniforms using spatial logic as the primary filter, while reserving expensive visual matching for necessary confirmations.

\subsubsection{Resource-Aware Scalability in Multi-Camera Pipelines}
\textbf{Insight from Literature:} Standard multi-camera tracking implementations often deploy independent detection and tracking instances for each camera feed. This 1-to-1 topology results in linear resource scaling, where computational overhead increases directly with the number of cameras, quickly saturating GPU memory.
\par
\textbf{Our Adaptation:} We devised a \textbf{One-to-Many} architecture to address this bottleneck. Instead of redundant model instances, we utilize a \textbf{Single Shared Detection Model} that multiplexes frames from all active cameras. This feeds into a \textbf{Custom ByteTrack Implementation} designed to manage independent tracking states for multiple camera contexts within a single execution thread. This design allows our system to scale to additional camera feeds with constant model memory overhead, significantly improving efficiency for campus-wide deployment.

In conclusion, while existing systems have made significant progress in multi-camera tracking, there remains a gap in solutions that effectively integrate persistent identity tracking, environmental adaptation, and interactive spatial mapping for campus applications. Our \usevar{\srsTitle} addresses this gap by combining state-of-the-art computer vision techniques designed for practical deployment in real-world environments.

Unlike existing systems that prioritize either single-camera performance or algorithmic benchmarks, SpotOn differentiates itself through a system-level integration of cross-camera identity preservation and unified spatial visualization. Commercial solutions often remain constrained to camera-centric views, while research implementations typically lack deployment-ready integration and usability. By combining identity persistence across non-overlapping cameras with spatial trajectory mapping and a user-centered interface, SpotOn introduces a practical reinterpretation of multi-camera tracking focused on operational effectiveness rather than isolated model performance.