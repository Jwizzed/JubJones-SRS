\chapter{Software Architecture Design}
\label{chap:software-architecture-design}

\section{Software Architecture}
\label{section:software-architecture}

The \usevar{\srsTitle} backend is built upon a \textbf{Layered Architecture} pattern using the \textbf{FastAPI} framework. This design promotes separation of concerns, maintainability, and scalability. The system is structured into clearly defined layers, each responsible for specific aspects of the data processing pipeline, from handling HTTP/WebSocket requests to executing complex AI logic and managing data persistence.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/jubjones/layer_architecture.png}
    \caption{Layered Architecture Diagram}
    \label{fig:backend-layers}
\end{figure}

The architecture consists of the following key layers:

\subsection{API Layer (Presentation)}
\label{subsection:api-layer}
The API Layer is the entry point for all external interactions. It is responsible for routing requests, validating input data using Pydantic schemas, and handling authentication/authorization.
\begin{itemize}
    \item \textbf{Technology:} FastAPI Routers (`APIRouter`), WebSockets.
    \item \textbf{Responsibilities:}
    \begin{itemize}
        \item \textbf{HTTP Endpoints:} RESTful APIs for environment management, analytics queries, and system control.
        \item \textbf{WebSocket Endpoints:} Real-time bi-directional communication for pushing detection metadata and receiving client heartbeats.
        \item \textbf{Middleware:} Handles cross-cutting concerns such as CORS, logging, and security headers.
    \end{itemize}
\end{itemize}

\subsection{Service Layer (Application Logic)}
\label{subsection:service-layer}
The Service Layer contains the core business logic and orchestrates the data flow between the API and Infrastructure layers. This is where the AI processing pipeline resides.
\begin{itemize}
    \item \textbf{Key Components:}
    \begin{itemize}
        \item \textbf{DetectionVideoService:} The central orchestrator that manages video stream processing, invoking object detectors (RT-DETR) and trackers.
        \item \textbf{HomographyService:} Manages the transformation of 2D camera coordinates to 2D map coordinates using pre-computed homography matrices.
        \item \textbf{FeatureExtractionService:} Wraps the FastReID model to extract appearance embeddings for person re-identification.
        \item \textbf{AnalyticsEngine:} Aggregates real-time data into meaningful metrics (e.g., people counting, dwell time).
    \end{itemize}
\end{itemize}

\subsection{Infrastructure Layer (Data Access)}
\label{subsection:infrastructure-layer}
The Infrastructure Layer abstracts the underlying data storage technologies, providing a clean API for the Service Layer to persist and retrieve data.
\begin{itemize}
    \item \textbf{Components:}
    \begin{itemize}
        \item \textbf{Repositories:} The `TrackingRepository` manages CRUD operations for tracking data, abstracting SQL queries.
        \item \textbf{Database:} \textbf{TimescaleDB} (PostgreSQL extension) is used for efficient storage and querying of time-series tracking data.
        \item \textbf{Cache:} \textbf{Redis} is utilized for high-speed caching of Re-ID embeddings and transient application state.
    \end{itemize}
\end{itemize}

\subsection{Domain/Model Layer}
\label{subsection:domain-model-layer}
This layer defines the data structures and business entities used throughout the application, ensuring type safety and consistency.
\begin{itemize}
    \item \textbf{Pydantic Models:} Used for data validation and serialization in the API layer (e.g., `Detection`, `Track`, `AnalyticsSummary`).
    \item \textbf{SQLAlchemy Models:} Mapped to database tables for ORM-based data access.
\end{itemize}

% However, some clarification may be needed as to help readers understand:

% \begin{itemize}
%     \item Stall can aggregate reviews from MenuItem directly, so no need to associate Review with Stall.
%     \item DietaryRestriction is considered separated from UserProfile. The benefit is that DietaryRestriction can be reused for other users
%     and we can specify common personal diets like: Vegetarian, Halal etc.
% \end{itemize}

% \section{Design Class Diagram}
% \label{section:design-class-diagram}

% \begin{figure}[h!]
%     \centering
%     % \includesvg[width=\textwidth,height=0.7\textheight,keepaspectratio]{kueater/class_diagram.svg}
%     \caption{Class Diagram of KU Eater}
%     \label{fig:class-diagram}
% \end{figure}

% Class diagram (Figure \ref{fig:class-diagram}) represents a structure of application that relates closely to real implementation. The diagram
% shows mainly the Data Objects that KU Eater must keep track of. Class Diagram is also expanded upon the Domain Model (see Figure \ref{fig:domain-model})
% making sure that the integrity of business knowledge is intact.

\section{Sequence Diagram}
\label{section:sequence-diagram}

The sequence diagram visualizes the real-time interaction between system components during the tracking process. It illustrates how data flows from the video source through the AI processing pipeline and finally to the user interface.

\subsection{Real-Time Tracking Process}
% Mermaid Source:
% sequenceDiagram
%     participant Camera as Camera Feed
%     participant VideoSvc as Video Service
%     participant Detector as Object Detector (RT-DETR)
%     participant Tracker as Tracker (ByteTrack)
%     participant HomoSvc as Homography Service
%     participant ReIDSvc as Re-ID Service
%     participant Redis as Redis Cache
%     participant WSManager as WebSocket Manager
%     participant Client as Client UI
%
%     loop Every Frame
%         Camera->>VideoSvc: Stream Frame (MJPEG/RTSP)
%         VideoSvc->>Detector: Input Frame
%         Detector-->>VideoSvc: Detections (BBox, Conf)
%         
%         VideoSvc->>Tracker: Update Tracks(Detections)
%         Tracker-->>VideoSvc: Temporal IDs
%         
%         par Spatial Mapping
%             VideoSvc->>HomoSvc: Map(Pixel Coords)
%             HomoSvc-->>VideoSvc: Floor Plan Coords (x,y)
%         and Re-Identification (If Needed)
%             opt New Track or Zone Crossing
%                 VideoSvc->>ReIDSvc: Extract Features(Crop)
%                 ReIDSvc->>Redis: Query Gallery(Embedding)
%                 Redis-->>ReIDSvc: Global ID Match/New ID
%                 ReIDSvc-->>VideoSvc: Global ID
%             end
%         end
%
%         VideoSvc->>WSManager: Publish Metadata (GlobalID, XY, BBox)
%         WSManager->>Client: WebSocket Update
%         Client->>Client: Render Overlay & Map
%     end
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth,height=0.6\textheight,keepaspectratio]{assets/jubjones/sequence_diagram.png}
    \caption{Sequence Diagram for Real-Time Multi-Camera Tracking}
    \label{fig:seq-tracking-process}
\end{figure}

Figure \ref{fig:seq-tracking-process} depicts the flow when a detection task is active:
1.  The \textbf{Video Service} ingests frames from the camera feed.
2.  Frames are passed to the \textbf{Object Detector} (RT-DETR) to identify individuals.
3.  Detections are sent to the \textbf{Tracker} (ByteTrack) to assign temporal IDs.
4.  The \textbf{Homography Service} maps pixel coordinates to the 2D floor plan.
5.  If a track crosses into a new zone or camera view, the \textbf{Re-ID Service} extracts features and queries the \textbf{Redis} cache for a global identity match.
6.  The aggregated tracking metadata (Global ID, Location, Bounding Box) is pushed to the \textbf{WebSocket Manager}.
7.  The \textbf{Client UI} receives the update and renders the bounding boxes and map markers in real-time.

\newpage

\section{Pipeline}
\label{section:pipeline}

\subsection{AI Processing Pipeline}
% Mermaid Source:
% flowchart TD
%     %% Nodes
%     A[Ingestion] --> B[Object Detection]
%     B --> C[Intra-Camera Tracking]
%     C --> D{Zone Crossing?}
%     
%     %% Spatial Mapping Path
%     C --> E[Spatial Mapping]
%     E --> G[Broadcast & Persistence]
%
%     %% Re-ID Path
%     D -- Yes --> F[Re-Identification]
%     D -- No --> G
%     
%     F --> F1[Feature Extraction]
%     F1 --> F2[Gallery Matching]
%     F2 --> G
%
%     %% Descriptions
%     A:::stage
%     B:::stage
%     C:::stage
%     E:::stage
%     F:::stage
%     G:::stage
%
%     classDef stage fill:#f9f,stroke:#333,stroke-width:2px;
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth,height=0.4\textheight,keepaspectratio]{assets/jubjones/pipeline.png}
    \caption{Data Processing Pipeline for SpotOn}
    \label{fig:pipeline-processing}
\end{figure}

The \usevar{\srsTitle} system employs a strictly ordered pipeline to transform raw video data into actionable tracking insights. As shown in Figure \ref{fig:pipeline-processing}, the pipeline consists of the following stages:

\begin{enumerate}[leftmargin=80pt]
    \item \textbf{Ingestion:} Video frames are extracted from source files or streams (RTSP/MJPEG) and pre-processed (resized, normalized) for model input.
    \item \textbf{Object Detection:} The RT-DETR model analyzes each frame to output bounding boxes and confidence scores for all detected persons.
    \item \textbf{Intra-Camera Tracking:} The ByteTrack algorithm processes detections to maintain identity consistency across consecutive frames within a single camera view, handling short-term occlusions.
    \item \textbf{Spatial Mapping:} The center point of each tracked person's foot-box is transformed using a homography matrix to obtain real-world $(x, y)$ coordinates on the facility's floor plan.
    \item \textbf{Re-Identification (Re-ID):}
    \begin{itemize}
        \item \textbf{Feature Extraction:} For tracks crossing camera boundaries, the FastReID model extracts high-dimensional appearance embeddings.
        \item \textbf{Matching:} These embeddings are compared against a gallery of active tracks in Redis (using cosine similarity) to determine if the person matches a known Global ID.
    \end{itemize}
    \item \textbf{Broadcast \& Persistence:} The final resolved track data (Global ID, Map Coordinates) is persisted to TimescaleDB for history and broadcast via WebSocket for real-time visualization.
\end{enumerate}

\section{Schemas}
\label{section:data-schema}

To ensure consistent data exchange between the Python backend, the AI models, and the React frontend, \usevar{\srsTitle} defines strict data schemas. The following are the core schemas used in the system:

\subsection{Person Track}
\label{schema:person-track}
The fundamental unit of tracking data, representing a detected individual at a specific point in time.
\begin{verbatim}
{
    "track_id": int,
    "global_id": string,
    "bbox_xyxy": [
        float, float, float, float 
    ],
    "confidence": float,
    "map_coords": [
        float, float
    ],
    "is_focused": boolean
}
\end{verbatim}

\subsection{Environment Configuration}
\label{schema:environment}
Defines the setup for a tracking location (e.g., Campus or Factory).
\begin{verbatim}
{
    "environment_id": string,
    "name": string,
    "camera_count": int,
    "zone_count": int,
    "has_data": boolean,
    "last_updated": string
}
\end{verbatim}

\subsection{WebSocket Tracking Update}
\label{schema:ws-update}
The payload sent to the frontend for real-time visualization.
\begin{verbatim}
{
    "type": "tracking_update",
    "task_id": "uuid",
    "camera_id": "string",
    "timestamp": "ISO 8601 String",
    "camera_data": {
        "tracks": [ PersonTrack, ... ]
    }
}
\end{verbatim}

\section{AI Component}
\label{section:ai-component}

\subsection{Overview of the AI Component}
\label{subsection:ai-component-overview}

The AI component of our system is designed to handle object detection, tracking, and re-identification (Re-ID) for \textbf{real-time} processing of video feeds. This component serves as the core intelligence of the application, enabling it to recognize and track objects across multiple camera views live. The system prioritizes \textbf{low-latency real-time processing} to support immediate situational awareness, optimized for high-throughput concurrency using our \textbf{One-to-Many} architecture. The AI component integrates with other system modules by streaming analysis through the following primary communication methods:
\begin{itemize}
    \item \textbf{MJPEG Streaming:} For delivering raw video frames to the frontend with minimal latency.
    \item \textbf{WebSocket:} For pushing real-time detection and tracking metadata overlays.
    \item \textbf{REST API:} For handling control commands and configuration requests.
\end{itemize}
This multi-protocol approach supports the system's overall objectives of accurate object tracking and identification in real-time.

\subsection{AI Model Description}
\label{subsection:ai-model-description}

\subsubsection{Object Detection}
\label{subsubsection:object-detection}

\begin{itemize}
    \item \textbf{Detection Model:} RT-DETR (Real-Time Detection Transformer) - Selected for its balance of speed and accuracy, specifically its ability to handle crowded scenes better than traditional YOLO-based approaches. This model is responsible for identifying "person" objects within each camera frame.
    \item \textbf{Key Features:} End-to-end detection without NMS post-processing, flexible speed tuning via decoder layers
    \item \textbf{Function:} Responsible for identifying and localizing objects (people) within video frames
\end{itemize}

\subsubsection{Tracking System}
\label{subsubsection:tracking-system}

\begin{itemize}
    \item \textbf{Model Type:} Custom ByteTrack Implementation (Multi-Context)
    \item \textbf{Architecture:} Modified ByteTrack algorithm designed for One-to-Many scalability
    \item \textbf{Key Features:} Handles multiple camera streams using a single detector instance. It acts as a centralized state manager, maintaining independent tracking histories for infinite camera contexts simultaneously without instantiating separate trackers for each feed.
    \item \textbf{Function:} Maintains identity consistency of detected objects across consecutive frames for all active cameras efficiently.
\end{itemize}

\subsubsection{Space-Based Location Prediction}
\label{subsubsection:space-based-matching}

\begin{itemize}
    \item \textbf{Type:} Algorithmic approach (not pure ML)
    \item \textbf{Purpose:} Handle overlapping camera views where multiple cameras observe the same physical space from different angles
    \item \textbf{Method:} Convert 3D location from Camera A into 2D floor plan coordinates using homography, then map back to 3D coordinates in Camera B to predict the person's location
    \item \textbf{Function:} Enables intra-frame global ID assignment for persons visible in overlapping camera regions
\end{itemize}

\subsubsection{Re-Identification (Re-ID) System}
\label{subsubsection:reid-system}

\begin{itemize}
    \item \textbf{Model Type:} FastReID deep convolutional neural network
    \item \textbf{Architecture:} Specialized deep learning network that extracts discriminative features for person re-identification
    \item \textbf{Key Features:} Supports open-set Re-ID methods, critical for real-world applications where previously unseen identities may appear
    \item \textbf{Function:} Associates identities across different camera views and non-consecutive frames (across-frame matching)
\end{itemize}

The selection of the Re-Identification model was informed by comparative evaluation using standard Re-ID benchmarks on the MTMMC dataset, including Rank-1 accuracy and mean Average Precision (mAP).

Based on this evaluation, FastReID was chosen for its strong discriminative performance and modular design, allowing it to be reliably integrated as a conditional identity confirmation component within the overall tracking pipeline.

\subsection{Dataset Information}
\label{subsection:dataset-information}

\subsubsection{Source}
\label{subsubsection:dataset-source}

The MTMMC (Multi-Target Multi-Camera) dataset is utilized for training and evaluating our AI models. This dataset is collected from two distinct environments: a campus and a factory, designed to provide a challenging testbed for real-world surveillance and tracking scenarios.

\subsubsection{Size and Composition}
\label{subsubsection:dataset-composition}

\begin{itemize}
    \item \textbf{Total Scenarios:} 25 scenarios (13 campus, 12 factory)
    \item \textbf{Number of Cameras:} 16 multi-modal cameras (RGB and thermal)
    \item \textbf{Total Frames:} Approximately 3.05 million frames
    \item \textbf{Frame Rate:} 23 frames per second
    \item \textbf{Video Duration:} Approximately 5.5 minutes per scenario
    \item \textbf{Dataset Split:}
    \begin{itemize}
        \item \textit{Training Set:} 14 scenarios (7 campus, 7 factory)
        \item \textit{Validation Set:} 5 scenarios (3 factory, 2 campus)
        \item \textit{Test Set:} 6 scenarios (2 factory, 4 campus)
    \end{itemize}
\end{itemize}

\subsubsection{Technical Specifications}
\label{subsubsection:dataset-technical-specs}

\begin{itemize}
    \item \textbf{Resolution:} RGB: 1920x1080 pixels, Thermal: 320x240 pixels
    \item \textbf{Annotation Process:} Semi-automatic labeling with manual correction
    \item \textbf{Multi-Modal Data:} RGB and thermal data are spatially aligned and temporally synchronized
\end{itemize}

\subsubsection{Key Features}
\label{subsubsection:dataset-key-features}

\begin{itemize}
    \item \textbf{Multi-Camera Views:} Multiple camera angles to train models for cross-view recognition
    \item \textbf{Environmental Diversity:} Different times of day, weather conditions, and seasons
    \item \textbf{Challenging Scenarios:} Occlusions, varying lighting conditions, and noisy detections
    \item \textbf{Thermal Data:} Enhances tracking accuracy in scenarios with limited visibility
\end{itemize}

\subsection{Training Process}
\label{subsection:training-process}

\subsubsection{Training Environment}
\label{subsubsection:training-environment}

\begin{itemize}
    \item \textbf{Hardware:} NVIDIA RTX 4060 GPU
    \item \textbf{Software:} PyTorch deep learning framework
    \item \textbf{Computing Resources:} Local workstation for model fine-tuning
\end{itemize}

\subsubsection{Hyperparameters}
\label{subsubsection:hyperparameters}

\begin{itemize}
    \item \textbf{Learning Rate:} 
    \begin{itemize}
        \item RT-DETR: 0.0001 (default)
        \item ByteTrack and FastReID: 0.001
    \end{itemize}
    \item \textbf{Batch Size:} 
    \begin{itemize}
        \item RT-DETR: 16
        \item ByteTrack and FastReID: 16
    \end{itemize}
    \item \textbf{Number of Epochs:}
    \begin{itemize}
        \item RT-DETR: Fine-tuned on MTMMC dataset
        \item ByteTrack and FastReID: 50 epochs
    \end{itemize}
    \item \textbf{Optimizer:} AdamW with weight decay of 0.0001
\end{itemize}

\subsubsection{Data Augmentation}
\label{subsubsection:data-augmentation}

\begin{itemize}
    \item \textbf{Techniques:} Random cropping, horizontal flipping, color jittering, random erasing
    \item \textbf{Purpose:} Prevent overfitting and improve model generalization
    \item \textbf{Implementation:} Applied during training using PyTorch transformations
\end{itemize}

\subsection{Evaluation Metrics}
\label{subsection:evaluation-metrics}

\subsubsection{Object Detection Metrics}
\label{subsubsection:detection-metrics}

\begin{itemize}
    \item \textbf{Precision:} Measures the accuracy of positive predictions
    \item \textbf{Recall:} Measures the ability to find all relevant instances
    \item \textbf{Average Precision (AP):} Summarizes the precision-recall curve
    \item \textbf{Confidence Threshold:} 0.5 for determining true positives
\end{itemize}

\subsubsection{Tracking Metrics}
\label{subsubsection:tracking-metrics}

\begin{itemize}
    \item \textbf{Multiple Object Tracking Accuracy (MOTA):} Measures overall tracking accuracy, considering false positives, missed detections, and identity switches
    \item \textbf{Multiple Object Tracking Precision (MOTP):} Measures the precision of tracking results
    \item \textbf{ID F1 Score (IDF1):} Measures the identity preservation capabilities
    \item \textbf{Identity Switches (IDSW):} Counts the number of identity switches
\end{itemize}

\subsubsection{Re-Identification Metrics}
\label{subsubsection:reid-metrics}

\begin{itemize}
    \item \textbf{Rank-1 Accuracy:} Percentage of queries where the correct match is the top result
    \item \textbf{Mean Average Precision (mAP):} Average precision across all ranks
    \item \textbf{Cumulative Matching Characteristic (CMC) Curve:} Measures the probability of finding the correct match within the top-k matches
\end{itemize}

\subsection{Best Practices}
\label{subsection:best-practices}

\subsubsection{Model Versioning}
\label{subsubsection:model-versioning}

\begin{itemize}
    \item \textbf{Version Control:} Each trained model is versioned and stored in a centralized repository
    \item \textbf{Documentation:} Detailed documentation maintained for each model version, including:
    \begin{itemize}
        \item Hyperparameters used for training
        \item Training and validation performance metrics
        \item Dataset version and preprocessing details
        \item Dependencies and environment specifications
    \end{itemize}
    \item \textbf{Backup Strategy:} Regular backups of model weights and checkpoints to prevent data loss
\end{itemize}

\subsubsection{Continuous Learning}
\label{subsubsection:continuous-learning}

\begin{itemize}
    \item \textbf{Data Updates:} Periodic dataset updates with new data to keep models current
    \item \textbf{Model Fine-tuning:} Regular model adjustments to adapt to changes in data distribution
    \item \textbf{Performance Monitoring:} Continuous evaluation of model performance on validation data
    \item \textbf{Feedback Loop:} Retrieval accuracy will be improved using \textbf{measurement and feedback loops} from system operators.
\end{itemize}

\subsubsection{Ethical Considerations}
\label{subsubsection:ethical-considerations}

\begin{itemize}
    \item \textbf{Bias Mitigation:} Regular audits to ensure models do not exhibit biases based on gender, race, or other sensitive attributes
    \item \textbf{Transparency:} Model decision-making processes made transparent to ensure accountability
    \item \textbf{Privacy Protection:} Strict adherence to privacy regulations regarding the use of personal data
    \item \textbf{Security Measures:} Implementation of robust security protocols to prevent unauthorized access to models and data
\end{itemize}

\subsection{Integration and Deployment}
\label{subsection:integration-deployment}

\subsubsection{APIs}
\label{subsubsection:apis}

\begin{itemize}
    \item \textbf{RESTful APIs:} 
    \begin{itemize}
        \item \textit{/api/detection:} Endpoint for object detection requests
        \item \textit{/api/tracking:} Endpoint for tracking requests
        \item \textit{/api/reid:} Endpoint for re-identification requests
        \item \textit{/api/batch-process:} Endpoint for batch processing of video data
    \end{itemize}
    \item \textbf{WebSocket:} 
    \begin{itemize}
        \item \textit{ws://hostname/tracking:} Socket for real-time tracking updates
        \item \textit{ws://hostname/notification:} Socket for system notifications
    \end{itemize}
    \item \textbf{Documentation:} Comprehensive API documentation using OpenAPI specification
\end{itemize}

\subsubsection{Deployment Strategy}
\label{subsubsection:deployment-strategy}

\begin{itemize}
    \item \textbf{Containerization:} 
    \begin{itemize}
        \item Docker containers for encapsulating each AI model
        \item Docker Compose for managing multi-container deployments
    \end{itemize}
    \item \textbf{Orchestration:} 
    \begin{itemize}
        \item To handle real-time processing of multiple camera feeds at scale, the system uses \textbf{Kubernetes} for service orchestration.
        \item The system features a \textbf{modular architecture} where detection and tracking services are containerized using \textbf{Docker}.
        \item We conducted a \textbf{risk and impact assessment} regarding the privacy implications and computational bottlenecks of multi-camera tracking.
        \item Horizontal Pod Autoscaling for handling varying loads
    \end{itemize}
    \item \textbf{CI/CD Pipeline:} 
    \begin{itemize}
        \item Automated testing and deployment using GitHub CI/CD
        \item Development will follow an \textbf{evaluation-driven development} approach, incorporating automated test cases, \textbf{regression testing} for the models, and \textbf{load/performance tests} to ensure real-time latency.
    \end{itemize}
\end{itemize}

\subsubsection{Database and Caching}
\label{subsubsection:database-caching}

\begin{itemize}
    \item \textbf{Scalable Knowledge Base:} 
    \begin{itemize}
        \item The system implements a scalable knowledge base using hybrid search, combining keyword filtering (time/location) with vector search (FastReID embeddings stored in a vector database).
        \item Stores time-series data such as tracking events.
    \end{itemize}
    \item \textbf{Data Governance:}
    \begin{itemize}
        \item The system enforces strict metadata governance and maintains data lineage for all captured video segments to ensure security compliance.
    \end{itemize}
    \item \textbf{Redis:} 
    \begin{itemize}
        \item Implements real-time caching for tracking results
        \item Configured with appropriate eviction policies and persistence
    \end{itemize}
    \item \textbf{Data Retention:} 
    \begin{itemize}
        \item Configurable retention periods for different types of data
        \item Automated data archiving for long-term storage
    \end{itemize}
\end{itemize}

\subsection{Conclusion}
\label{subsection:conclusion}

The AI component is a critical element of our system, enabling sophisticated \textbf{real-time} processing of video feeds for object detection, tracking, space-based location prediction, and re-identification. By leveraging state-of-the-art models like RT-DETR, ByteTrack, and FastReID, trained on the comprehensive MTMMC dataset, our system achieves high accuracy even in challenging scenarios. The training methodology using NVIDIA RTX 4060 GPU, combined with continuous learning approaches and ethical considerations, ensures that the AI component remains effective and responsible. The space-based location prediction algorithm enables intra-frame global ID assignment for overlapping camera views, complementing the Re-ID model for across-frame matching. The seamless integration with other system components via MJPEG, WebSocket, and REST APIs provides a scalable and reliable real-time solution for our users.