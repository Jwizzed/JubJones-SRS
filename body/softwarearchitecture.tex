\chapter{Software Architecture Design}
\label{chap:software-architecture-design}

\section{Domain Model}
\label{section:domain-model}

% \begin{figure}[h!]
%     \centering
%     % \includesvg[width=1\textwidth]{kueater/domain_model.svg}
%     \caption{Domain Model of KU Eater}
%     \label{fig:domain-model}
% \end{figure}

% The domain model of KU Eater (Figure \ref{fig:domain-model}) shows the business knowledge, any technicalities are abstracted into their most simplest form. (Recommendation Engine for example)

% However, some clarification may be needed as to help readers understand:

% \begin{itemize}
%     \item Stall can aggregate reviews from MenuItem directly, so no need to associate Review with Stall.
%     \item DietaryRestriction is considered separated from UserProfile. The benefit is that DietaryRestriction can be reused for other users
%     and we can specify common personal diets like: Vegetarian, Halal etc.
% \end{itemize}

% \section{Design Class Diagram}
% \label{section:design-class-diagram}

% \begin{figure}[h!]
%     \centering
%     % \includesvg[width=\textwidth,height=0.7\textheight,keepaspectratio]{kueater/class_diagram.svg}
%     \caption{Class Diagram of KU Eater}
%     \label{fig:class-diagram}
% \end{figure}

% Class diagram (Figure \ref{fig:class-diagram}) represents a structure of application that relates closely to real implementation. The diagram
% shows mainly the Data Objects that KU Eater must keep track of. Class Diagram is also expanded upon the Domain Model (see Figure \ref{fig:domain-model})
% making sure that the integrity of business knowledge is intact.

\section{Sequence Diagram}
\label{section:sequence-diagram}

% % Sequence diagrams visualize interaction between the user and the system. The figures below show how components in the system will interact with
% % each other and the end-user, while the users can dispatch messages or inputs into KU Eater.

% \subsection{Default Recommendation Process}
% \begin{figure}[h!]
%     \centering
%     % \includesvg[width=\textwidth,height=0.4\textheight,keepaspectratio]{kueater/sequence_diagram_anonymous_recommendations.svg}
%     \caption{Sequence Diagram for Default Recommendation Process}
%     \label{fig:seq-default-recommendation-process}
% \end{figure}

% Figure \ref{fig:seq-default-recommendation-process} represents a sequence diagram for a process when a guest user is browsing KU Eater. When a user
% starts to use KU Eater, the controller in the application layer will invoke the machine learning layer without specific parameters and generate new recommendations.

% Once the recommendations are finished generating, the application requests list of recommendations and renders them back to user.

% \subsection{User Recommendation Process}
% \begin{figure}[h!]
%     \centering
%     % \includesvg[width=\textwidth,height=0.4\textheight,keepaspectratio]{kueater/sequence_diagram_user_recommendations.svg}
%     \caption{Sequence Diagram for Authenticated User Recommendation Process}
%     \label{fig:seq-user-recommendation-process}
% \end{figure}

% Figure \ref{fig:seq-user-recommendation-process} represents a sequence diagram for a process when authenticated user logs on to browse on KU Eater.
% When a user, starts to use KU Eater, the controller in the application layer will fetch the user profile from the domain layer and pass the attributes in the profile to
% the machine learning layer with user profile as the parameters. The recommendation engine will generate a list of possible recommendations.

% Once the recommendations are finished generating, it is re-ranked, filtered and evaluated by considering the properties in user profile: Liked menu items, reviewed menu items, disliked menu items and dietary restrictions. The final set of recommendations is rendered back to user.

% %\section{Algorithm}
% %\label{section:algorithm}
% %<TIP: Optional, If you are working on a research project that proposes a new
% %algorithm, you can describe your algorithm here. It can be in the form of
% %pseudocode or any diagram that you deem appropriate./>

% \newpage

\section{Pipeline}
\label{section:pipeline}

% \subsection{Modelling the Recommendation Engine}
% \begin{figure}[h!]
%     \centering
%     % \includesvg[height=0.3\textheight,keepaspectratio]{kueater/algo_modelling.svg}
%     \caption{Pipeline for Modelling the Recommendation Engine}
%     \label{fig:pipeline-for-modelling}
% \end{figure}

% Our recommendation engine uses content-based filtering and item-based collaborative filtering
% with K-nearest neighbor (kNN) as the backbone of the engine. \cite{singhanddwivedi:2023}
% The model needs to be fed relevant data such as Menu Categories, Menu Items and Stalls in which they exist in
% the database. As shown in, figure \ref{fig:pipeline-for-modelling}.

% The following data types are used in the modelling process:

% \begin{enumerate}[leftmargin=80pt]
%     \item \textbf{Menu items:} A structured data type containing menu name, ingredients and cooking method.
%     \item \textbf{Menu categories:} A set of keywords that corresponds to menu item types, cuisines, cooking method etc.
%     The categories are used to tag the menu items to further enhance decision making for model and simplify its
%     similarity calculation.
%     \item \textbf{Stalls:} A structured data type containing stall name, menu items a stall offers etc.
%     It is used to multiply menu entries to a number of stalls, so that we can further score specific item from reviews.
% \end{enumerate}

% The main objective of this recommendation system is to figure out what food items are relevant with inputted food items that user interacted with.
% In order to build the proper system, we must start with finding relevance between menu items by using \textit{Content-Based Filtering}.

% \begin{enumerate}[leftmargin=80pt]
%     \item We compile a table of menu items which has the following attributes, menu name, a list of ingredients and cooking method as a string.
%     \item The tags from Menu categories will automatically populate within each menu item.
%     \item We use cosine similarity technique to match any string-based attributes.
%     \item We turn ingredients feature into binary (Exists, Non-exists), in that way we can score the similarity of food items with existence of each ingredients.
%     \item Finally, we can then use cosine similarity measure to find distances between all food items.
% \end{enumerate}

% After having a precomputed similarity vectors of each food item, we can use them to determine recommendations for a user;
% by using \textit{Item-Based Collaborative Filtering}.

% \begin{enumerate}[leftmargin=80pt]
%     \item Once a user required new recommendations, a list of food items and stall is used to generate all possible recommendations.
%     \item Using the list of similarity scores from CBF, we can determine the most similar menu items from user preferences with cosine similarity.
%     \item We can take ratings of observed menu items and construct a user-item correlation set.
%     \item Using cosine similarity, we can determine the nearest neighbors which are other users with similar preferences.
%     \item We can use those other users' profile to see what menu item they're most likely to interact with, and use those as recommendations.
% \end{enumerate}

% \subsection{Predicting the Recommendations}
% \begin{figure}[h!]
%     \centering
%     % \includesvg[width=\textwidth,height=0.2\textheight,keepaspectratio]{kueater/algo_prediction.svg}
%     \caption{Pipeline for Predicting Recommendations}
%     \label{fig:pipeline-for-predicting}
% \end{figure}

% User profile contains information that can be used to predict their preferences: Items that user recently looked, items reviewed by user, items liked by user.
% These properties are used in the recommendation engine to predict an unordered list of possible recommendations.

% However, the recommendations are not refined towards user, so additional properties are used externally: Disliked items and dietary restrictions.
% "Disliked items" is used to reduce the appearance of disliked items, while "Dietary restrictions" is used to remove items from the list that users
% cannot have. As shown in, figure \ref{fig:pipeline-for-predicting}.

\section{Schemas}
\label{section:data-schema}

% KU Eater requires a lot of data from many sources as mentioned in \ref{subsection:data-acquisition}. In order to standardize and streamline the
% data acquisition and processing phases, schemas for each important data type must be clear. The following are the schemas for specific data types:

% \subsection{Menu Item}
% \label{schema:menu-item}
% \begin{verbatim}
%     menu_id: uid,
    
%     menu_name: string,

%     price: double,
    
%     ingredients: int<Ingredient>[],
%     # refers to ingredients in database.
    
%     tags: int[]
%     # refers to menu categories in database.
% \end{verbatim}

% \subsection{Ingredient}
% \label{schema:ingredient}
% \begin{verbatim}
%     ingredient_id: uid,
    
%     ingredient_name: string
% \end{verbatim}

% \subsection{Stall}
% \label{schema:stall}
% \begin{verbatim}
%     stall_id: uid,

%     stall_name: string,

%     items: uid<MenuItem>[]
%     # refers to what items they are selling.
% \end{verbatim}

% \subsection{User Profile}
% \label{schema:user-profile}
% \begin{verbatim}
%     profile_id: uid,
    
%     name: string,
    
%     liked: uid<MenuItem>[],
%     # refers to liked menu items.

%     disliked: uid<MenuItem>[],
%     # refers to disliked menu items.

%     restrictions: DietaryRestriction
% \end{verbatim}

% \subsection{Dietary Restriction}
% \label{schema:dietary-restriction}
% \begin{verbatim}
%     blacklist: uid<Ingredient>[]
%     # refers to ingredients prohibited
%     for consumption.
% \end{verbatim}

\section{AI Component}
\label{section:ai-component}

\subsection{Overview of the AI Component}
\label{subsection:ai-component-overview}

The AI component of our system is designed to handle object detection, tracking, and re-identification (Re-ID) for **real-time** processing of video feeds. This component serves as the core intelligence of the application, enabling it to recognize and track objects across multiple camera views live. The system prioritizes **low-latency real-time processing** to support immediate situational awareness, optimized for high-throughput concurrency using our **One-to-Many** architecture. The AI component integrates with other system modules by streaming analysis through WebSocket connections, supporting the system's overall objectives of accurate object tracking and identificaiton in real-time.

\subsection{AI Model Description}
\label{subsection:ai-model-description}

\subsubsection{Object Detection}
\label{subsubsection:object-detection}

\begin{itemize}
    \item \textbf{Model Type:} RT-DETR (Real-Time DEtection TRansformer) deep learning model
    \item \textbf{Architecture:} Transformer-based detection model with efficient hybrid encoder and uncertainty-minimal query selection
    \item \textbf{Key Features:} End-to-end detection without NMS post-processing, flexible speed tuning via decoder layers
    \item \textbf{Function:} Responsible for identifying and localizing objects (people) within video frames
\end{itemize}

\subsubsection{Tracking System}
\label{subsubsection:tracking-system}

\begin{itemize}
    \item \textbf{Model Type:} Custom ByteTrack Implementation (Multi-Context)
    \item \textbf{Architecture:} Modified ByteTrack algorithm designed for One-to-Many scalability
    \item \textbf{Key Features:} Handles multiple camera streams using a single detector instance. It acts as a centralized state manager, maintaining independent tracking histories for infinite camera contexts simultaneously without instantiating separate trackers for each feed.
    \item \textbf{Function:} Maintains identity consistency of detected objects across consecutive frames for all active cameras efficiently.
\end{itemize}

\subsubsection{Space-Based Location Prediction}
\label{subsubsection:space-based-matching}

\begin{itemize}
    \item \textbf{Type:} Algorithmic approach (not pure ML)
    \item \textbf{Purpose:} Handle overlapping camera views where multiple cameras observe the same physical space from different angles
    \item \textbf{Method:} Convert 3D location from Camera A into 2D floor plan coordinates using homography, then map back to 3D coordinates in Camera B to predict the person's location
    \item \textbf{Function:} Enables intra-frame global ID assignment for persons visible in overlapping camera regions
\end{itemize}

\subsubsection{Re-Identification (Re-ID) System}
\label{subsubsection:reid-system}

\begin{itemize}
    \item \textbf{Model Type:} FastReID deep convolutional neural network
    \item \textbf{Architecture:} Specialized deep learning network that extracts discriminative features for person re-identification
    \item \textbf{Key Features:} Supports open-set Re-ID methods, critical for real-world applications where previously unseen identities may appear
    \item \textbf{Function:} Associates identities across different camera views and non-consecutive frames (across-frame matching)
\end{itemize}

The selection of the Re-Identification model was informed by comparative evaluation using standard Re-ID benchmarks on the MTMMC dataset, including Rank-1 accuracy and mean Average Precision (mAP).

Based on this evaluation, FastReID was chosen for its strong discriminative performance and modular design, allowing it to be reliably integrated as a conditional identity confirmation component within the overall tracking pipeline.

\subsection{Dataset Information}
\label{subsection:dataset-information}

\subsubsection{Source}
\label{subsubsection:dataset-source}

The MTMMC (Multi-Target Multi-Camera) dataset is utilized for training and evaluating our AI models. This dataset is collected from two distinct environments: a campus and a factory, designed to provide a challenging testbed for real-world surveillance and tracking scenarios.

\subsubsection{Size and Composition}
\label{subsubsection:dataset-composition}

\begin{itemize}
    \item \textbf{Total Scenarios:} 25 scenarios (13 campus, 12 factory)
    \item \textbf{Number of Cameras:} 16 multi-modal cameras (RGB and thermal)
    \item \textbf{Total Frames:} Approximately 3.05 million frames
    \item \textbf{Frame Rate:} 23 frames per second
    \item \textbf{Video Duration:} Approximately 5.5 minutes per scenario
    \item \textbf{Dataset Split:}
    \begin{itemize}
        \item \textit{Training Set:} 14 scenarios (7 campus, 7 factory)
        \item \textit{Validation Set:} 5 scenarios (3 factory, 2 campus)
        \item \textit{Test Set:} 6 scenarios (2 factory, 4 campus)
    \end{itemize}
\end{itemize}

\subsubsection{Technical Specifications}
\label{subsubsection:dataset-technical-specs}

\begin{itemize}
    \item \textbf{Resolution:} RGB: 1920x1080 pixels, Thermal: 320x240 pixels
    \item \textbf{Annotation Process:} Semi-automatic labeling with manual correction
    \item \textbf{Multi-Modal Data:} RGB and thermal data are spatially aligned and temporally synchronized
\end{itemize}

\subsubsection{Key Features}
\label{subsubsection:dataset-key-features}

\begin{itemize}
    \item \textbf{Multi-Camera Views:} Multiple camera angles to train models for cross-view recognition
    \item \textbf{Environmental Diversity:} Different times of day, weather conditions, and seasons
    \item \textbf{Challenging Scenarios:} Occlusions, varying lighting conditions, and noisy detections
    \item \textbf{Thermal Data:} Enhances tracking accuracy in scenarios with limited visibility
\end{itemize}

\subsection{Training Process}
\label{subsection:training-process}

\subsubsection{Training Environment}
\label{subsubsection:training-environment}

\begin{itemize}
    \item \textbf{Hardware:} NVIDIA RTX 2060 GPU
    \item \textbf{Software:} PyTorch deep learning framework
    \item \textbf{Computing Resources:} Local workstation for model fine-tuning
\end{itemize}

\subsubsection{Hyperparameters}
\label{subsubsection:hyperparameters}

\begin{itemize}
    \item \textbf{Learning Rate:} 
    \begin{itemize}
        \item RT-DETR: 0.0001 (default)
        \item ByteTrack and FastReID: 0.001
    \end{itemize}
    \item \textbf{Batch Size:} 
    \begin{itemize}
        \item RT-DETR: 16
        \item ByteTrack and FastReID: 16
    \end{itemize}
    \item \textbf{Number of Epochs:}
    \begin{itemize}
        \item RT-DETR: Fine-tuned on MTMMC dataset
        \item ByteTrack and FastReID: 50 epochs
    \end{itemize}
    \item \textbf{Optimizer:} AdamW with weight decay of 0.0001
\end{itemize}

\subsubsection{Data Augmentation}
\label{subsubsection:data-augmentation}

\begin{itemize}
    \item \textbf{Techniques:} Random cropping, horizontal flipping, color jittering, random erasing
    \item \textbf{Purpose:} Prevent overfitting and improve model generalization
    \item \textbf{Implementation:} Applied during training using PyTorch transformations
\end{itemize}

\subsection{Evaluation Metrics}
\label{subsection:evaluation-metrics}

\subsubsection{Object Detection Metrics}
\label{subsubsection:detection-metrics}

\begin{itemize}
    \item \textbf{Precision:} Measures the accuracy of positive predictions
    \item \textbf{Recall:} Measures the ability to find all relevant instances
    \item \textbf{Average Precision (AP):} Summarizes the precision-recall curve
    \item \textbf{Confidence Threshold:} 0.5 for determining true positives
\end{itemize}

\subsubsection{Tracking Metrics}
\label{subsubsection:tracking-metrics}

\begin{itemize}
    \item \textbf{Multiple Object Tracking Accuracy (MOTA):} Measures overall tracking accuracy, considering false positives, missed detections, and identity switches
    \item \textbf{Multiple Object Tracking Precision (MOTP):} Measures the precision of tracking results
    \item \textbf{ID F1 Score (IDF1):} Measures the identity preservation capabilities
    \item \textbf{Identity Switches (IDSW):} Counts the number of identity switches
\end{itemize}

\subsubsection{Re-Identification Metrics}
\label{subsubsection:reid-metrics}

\begin{itemize}
    \item \textbf{Rank-1 Accuracy:} Percentage of queries where the correct match is the top result
    \item \textbf{Mean Average Precision (mAP):} Average precision across all ranks
    \item \textbf{Cumulative Matching Characteristic (CMC) Curve:} Measures the probability of finding the correct match within the top-k matches
\end{itemize}

\subsection{Best Practices}
\label{subsection:best-practices}

\subsubsection{Model Versioning}
\label{subsubsection:model-versioning}

\begin{itemize}
    \item \textbf{Version Control:} Each trained model is versioned and stored in a centralized repository
    \item \textbf{Documentation:} Detailed documentation maintained for each model version, including:
    \begin{itemize}
        \item Hyperparameters used for training
        \item Training and validation performance metrics
        \item Dataset version and preprocessing details
        \item Dependencies and environment specifications
    \end{itemize}
    \item \textbf{Backup Strategy:} Regular backups of model weights and checkpoints to prevent data loss
\end{itemize}

\subsubsection{Continuous Learning}
\label{subsubsection:continuous-learning}

\begin{itemize}
    \item \textbf{Data Updates:} Periodic dataset updates with new data to keep models current
    \item \textbf{Model Fine-tuning:} Regular model adjustments to adapt to changes in data distribution
    \item \textbf{Performance Monitoring:} Continuous evaluation of model performance on validation data
    \item \textbf{Feedback Loop:} Incorporation of user feedback to improve model accuracy
\end{itemize}

\subsubsection{Ethical Considerations}
\label{subsubsection:ethical-considerations}

\begin{itemize}
    \item \textbf{Bias Mitigation:} Regular audits to ensure models do not exhibit biases based on gender, race, or other sensitive attributes
    \item \textbf{Transparency:} Model decision-making processes made transparent to ensure accountability
    \item \textbf{Privacy Protection:} Strict adherence to privacy regulations regarding the use of personal data
    \item \textbf{Security Measures:} Implementation of robust security protocols to prevent unauthorized access to models and data
\end{itemize}

\subsection{Integration and Deployment}
\label{subsection:integration-deployment}

\subsubsection{APIs}
\label{subsubsection:apis}

\begin{itemize}
    \item \textbf{RESTful APIs:} 
    \begin{itemize}
        \item \textit{/api/detection:} Endpoint for object detection requests
        \item \textit{/api/tracking:} Endpoint for tracking requests
        \item \textit{/api/reid:} Endpoint for re-identification requests
        \item \textit{/api/batch-process:} Endpoint for batch processing of video data
    \end{itemize}
    \item \textbf{WebSocket:} 
    \begin{itemize}
        \item \textit{ws://hostname/tracking:} Socket for real-time tracking updates
        \item \textit{ws://hostname/notification:} Socket for system notifications
    \end{itemize}
    \item \textbf{Documentation:} Comprehensive API documentation using OpenAPI specification
\end{itemize}

\subsubsection{Deployment Strategy}
\label{subsubsection:deployment-strategy}

\begin{itemize}
    \item \textbf{Containerization:} 
    \begin{itemize}
        \item Docker containers for encapsulating each AI model
        \item Docker Compose for managing multi-container deployments
    \end{itemize}
    \item \textbf{Orchestration:} 
    \begin{itemize}
        \item Kubernetes for container orchestration
        \item Horizontal Pod Autoscaling for handling varying loads
    \end{itemize}
    \item \textbf{CI/CD Pipeline:} 
    \begin{itemize}
        \item Automated testing and deployment using Jenkins
        \item Blue-green deployment strategy for zero-downtime updates
    \end{itemize}
\end{itemize}

\subsubsection{Database and Caching}
\label{subsubsection:database-caching}

\begin{itemize}
    \item \textbf{TimescaleDB:} 
    \begin{itemize}
        \item Stores time-series data such as tracking events
        \item Schema optimized for efficient querying of temporal data
    \end{itemize}
    \item \textbf{Redis:} 
    \begin{itemize}
        \item Implements real-time caching for tracking results
        \item Configured with appropriate eviction policies and persistence
    \end{itemize}
    \item \textbf{Data Retention:} 
    \begin{itemize}
        \item Configurable retention periods for different types of data
        \item Automated data archiving for long-term storage
    \end{itemize}
\end{itemize}

\subsection{Conclusion}
\label{subsection:conclusion}

The AI component is a critical element of our system, enabling sophisticated **real-time** processing of video feeds for object detection, tracking, space-based location prediction, and re-identification. By leveraging state-of-the-art models like RT-DETR, ByteTrack, and FastReID, trained on the comprehensive MTMMC dataset, our system achieves high accuracy even in challenging scenarios. The training methodology using NVIDIA RTX 2060 GPU, combined with continuous learning approaches and ethical considerations, ensures that the AI component remains effective and responsible. The space-based location prediction algorithm enables intra-frame global ID assignment for overlapping camera views, complementing the Re-ID model for across-frame matching. The seamless integration with other system components continuous WebSocket streaming provides a scalable and reliable real-time solution for our users.